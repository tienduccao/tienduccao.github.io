<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.41" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Spacy pretrain command &middot; Duc Cao</title>

  
  <link type="text/css" rel="stylesheet" href="https://tienduccao.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://tienduccao.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://tienduccao.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://tienduccao.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Duc Cao" />

  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://tienduccao.github.io/"><h1>Duc Cao</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://tienduccao.github.io/">Home</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>Spacy pretrain command</h1>
  <time datetime=2012-02-04T00:00:00Z class="post-date">Sat, Feb 4, 2012</time>
  <p><a href="https://explosion.ai/blog/spacy-v2-1">Spacy 2.1</a> released an interesting command <em>spacy pretrain</em>.
According to the creator of Spacy, this feature is especially useful when you have limited training data
for text classification and parsing task.
He used pretraining&rsquo;s output to training a text classifier on 1,000 samples and reported
a high F1-score of <a href="https://github.com/honnibal/spacy-pretrain-polyaxon#text-classification-results">87%</a>
on the test set consisting of 5,000 samples.</p>

<p>In order to get familiar with this technique, I ran a similar experiment on a new dataset.
All the code and data are available at
<a href="https://github.com/tienduccao/spacy-pretrain-polyaxon">https://github.com/tienduccao/spacy-pretrain-polyaxon</a>s,
under <em>lmao-imdb-1k</em> folder.</p>

<p>Firstly you need to convert your unlabled text into <a href="https://spacy.io/api/cli#pretrain-jsonl">.jsonl format</a>.
Then executing the command<code>spacy pretrain</code> to obtain the pre-trained weights.</p>

<pre><code>spacy pretrain corpus.jsonl en_vectors_web_lg weights
</code></pre>

<p>The details of this command could be found at <a href="https://spacy.io/api/cli#pretrain">https://spacy.io/api/cli#pretrain</a>.
This task took 24,5h for 832 iterations on GeForce GTX 1080.
Some pre-trained weights are stored
<a href="https://github.com/tienduccao/spacy-pretrain-polyaxon/tree/master/lmao-imdb-1k/weights">here</a>.</p>

<p>Without using the pre-trained weights (<code>python pretrain_textcat.py 96 2000</code>), we obtain an F1-score of 73.4%.</p>

<p>The next step is training a text classifier using pre-trained weights.</p>

<pre><code>python pretrain_textcat.py -t2v weights/model800.bin 96 200
</code></pre>

<p>Now the performance increases up to <strong>85.4%</strong>.</p>

<p>Using both pre-trained weights and word vectors, the classifier achieves even higher F1-score TODO.</p>

<!--The spacy pretrain command requires a word vectors model as part of the input, which it uses as the target output for each token. Instead of predicting a token's ID as a classification problem, we learn to predict the token's word vector. Inspired by names such as ELMo and BERT, we've termed this trick Language Modelling with Approximate Outputs (LMAO)-->

</div>


    </main>

    
  </body>
</html>
